{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Sparsifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparsifier allows to sparsify your network using different methods:\n",
    "- Local or Global: the choice of parameters removed is done per layer (e.g sparsity=50% removes half of parameters at each layer) or based on the whole network (e.g sparsity=50% removes half of the parameters of the network, whatever the layer they come from). \n",
    "- Weight, Kernel, Filter: granularity of the sparsifying.\n",
    "- Scheduling Function: scheduling applied to the removal of parameters. Scheduling supported by default can be found [here](https://docs.fast.ai/callback.html#Annealing-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ImageList.from_folder(path)\n",
    "                .split_by_folder(train='train', valid='val')\n",
    "                .label_from_folder()\n",
    "                .transform(get_transforms(), size=64)\n",
    "                .databunch(bs=64)\n",
    "                .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from fasterai.sparsifier_test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.vgg16_bn(), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.231394</td>\n",
       "      <td>5.494659</td>\n",
       "      <td>0.204841</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.858624</td>\n",
       "      <td>1.850971</td>\n",
       "      <td>0.338344</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.463806</td>\n",
       "      <td>1.214066</td>\n",
       "      <td>0.608662</td>\n",
       "      <td>00:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either prune after training by using the `prune` method. But you shouldn't expect great results like that as, even less important than others, the parameters you remove still have some importance overall, and by doing so, you don't give a chance to the network to recover from the loss of some of its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: remove 50% of the least important parameters\n",
    "sparsifier = Sparsifier(learn.model, granularity='weight', method='local', criteria='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsifier.prune(sparsity=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, pruning your network like that requires to retrain you model in order to allow it to recover from the removal of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[1.7763458, tensor(0.4104)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to work is by doing several iterations of **pruning->fine-tuning**. This process can be long and sensitive as you have to choose at each iteration, how much parameters to remove and a bad choice can lead to a completely broken network with no chance to recover.\n",
    "\n",
    "The goal of the `Sparsifier`function is rather to include pruning **into** the training process. By doing so, the time of the process is greatly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Weight Sparsifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove 50% of the parameters of VGG16 and see how the training behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.vgg16_bn(), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of weight until a sparsity of 50%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.241648</td>\n",
       "      <td>2.585599</td>\n",
       "      <td>0.181656</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.895520</td>\n",
       "      <td>2.031944</td>\n",
       "      <td>0.326369</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.550080</td>\n",
       "      <td>1.363338</td>\n",
       "      <td>0.551083</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Weights at epoch 0\n",
      "Sparsity at the end of epoch 0: 12.50%\n",
      "Sparsity at the end of epoch 1: 37.50%\n",
      "Sparsity at the end of epoch 2: 50.00%\n",
      "Final Sparsity: 50.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-3, callbacks=[SparsifyCallback(learn, sparsity=50, granularity='weight', method='local', criteria='l1', sched_func=annealing_cos)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our network has only half of its parameters that are used and still is able to achieve almost the same accuracy as when it was using 100% !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check that we correctly removed the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d 2: 50.00%\n",
      "Sparsity in Conv2d 5: 50.00%\n",
      "Sparsity in Conv2d 9: 50.00%\n",
      "Sparsity in Conv2d 12: 50.00%\n",
      "Sparsity in Conv2d 16: 50.00%\n",
      "Sparsity in Conv2d 19: 50.00%\n",
      "Sparsity in Conv2d 22: 50.00%\n",
      "Sparsity in Conv2d 26: 50.00%\n",
      "Sparsity in Conv2d 29: 50.00%\n",
      "Sparsity in Conv2d 32: 50.00%\n",
      "Sparsity in Conv2d 36: 50.00%\n",
      "Sparsity in Conv2d 39: 50.00%\n",
      "Sparsity in Conv2d 42: 50.00%\n"
     ]
    }
   ],
   "source": [
    "for k,m in enumerate(learn.model.modules()):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look closer to a single Convolution filter, we expect to see half of its values to be zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0887, -0.0848, -0.0687],\n",
      "         [ 0.0000, -0.0705,  0.0000],\n",
      "         [ 0.0427,  0.0000, -0.0550]],\n",
      "\n",
      "        [[-0.0509, -0.0000, -0.0000],\n",
      "         [-0.0804, -0.0552, -0.0000],\n",
      "         [ 0.1522,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0000, -0.0000,  0.0827],\n",
      "         [ 0.0540, -0.0000,  0.0697],\n",
      "         [-0.0000,  0.0000,  0.0566]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(learn.model.features[0].weight[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Filter Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try another way to prune our network, this time we will remove 20% of the least globally important filters. And we will try with another architecture, ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, models.resnet18(), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning of filter until a sparsity of 20%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.894946</td>\n",
       "      <td>1.995040</td>\n",
       "      <td>0.424713</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.277624</td>\n",
       "      <td>1.340366</td>\n",
       "      <td>0.572994</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.976437</td>\n",
       "      <td>0.914715</td>\n",
       "      <td>0.707261</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Weights at epoch 0\n",
      "Sparsity at the end of epoch 0: 5.00%\n",
      "Sparsity at the end of epoch 1: 15.00%\n",
      "Sparsity at the end of epoch 2: 20.00%\n",
      "Final Sparsity: 20.00\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, 1e-3, callbacks=[SparsifyCallback(learn, sparsity=20, granularity='filter', method='global', criteria='l1', sched_func=annealing_cos)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we expect to have different sparsities accross layers. This can give us a good indication of how deep in the network are important features extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d 1: 0.00%\n",
      "Sparsity in Conv2d 7: 0.00%\n",
      "Sparsity in Conv2d 10: 0.00%\n",
      "Sparsity in Conv2d 13: 0.00%\n",
      "Sparsity in Conv2d 16: 0.00%\n",
      "Sparsity in Conv2d 20: 0.00%\n",
      "Sparsity in Conv2d 23: 0.00%\n",
      "Sparsity in Conv2d 26: 0.00%\n",
      "Sparsity in Conv2d 29: 0.00%\n",
      "Sparsity in Conv2d 32: 0.00%\n",
      "Sparsity in Conv2d 36: 0.00%\n",
      "Sparsity in Conv2d 39: 0.00%\n",
      "Sparsity in Conv2d 42: 0.00%\n",
      "Sparsity in Conv2d 45: 0.00%\n",
      "Sparsity in Conv2d 48: 0.00%\n",
      "Sparsity in Conv2d 52: 30.08%\n",
      "Sparsity in Conv2d 55: 27.73%\n",
      "Sparsity in Conv2d 58: 0.00%\n",
      "Sparsity in Conv2d 61: 69.34%\n",
      "Sparsity in Conv2d 64: 60.35%\n"
     ]
    }
   ],
   "source": [
    "for k,m in enumerate(learn.model.modules()):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(f\"Sparsity in {m.__class__.__name__} {k}: {100. * float(torch.sum(m.weight == 0))/ float(m.weight.nelement()):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
