# fasterai
FasterAI: A repository for making smaller and faster models with the FastAI library.

It allows you to use techniques such as:

- Knowledge Distillation
- Pruning 
- Batch Normalization Folding
- Matrix Decomposition

---

## Usage:

### Knowledge Distillation

Used as a callback to make the student model train on soft-labels generated by a teacher model.

<blockquote>
<pre><b> KnowledgeDistillation(student:Learner, teacher:Learner)</b></pre>
<p style="font-size: 15px">
You only need to give to the callback function your student learner and your teacher learner. Behind the scenes, fasterai will take care of making your model train using knowledge distillation
</p>
</blockquote>

<br>

### Sparsify the network

Used as a callback, will iteratively replace the lowest-norm parameters by zeroes. More information in this [blog post](https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html)

<blockquote>
    <pre><b>SparsifyCallback(learn, sparsity, granularity, method, criteria, sched_func)</b></pre>

<ul>
<li style="font-size:15px"><b>sparsity</b>: the percentage of sparsity that you want in your network </li>
<li style="font-size:15px"><b>granularity</b>: on what granularity you want the sparsification to be operated (currently supported: <code>weight</code>, <code>kernel</code>, <code>filter</code>)</li>
<li style="font-size:15px"><b>method</b>: either <code>local</code> or <code>global</code>, will affect the selection of parameters to be choosen in each layer independently (<code>local</code>) or on the whole network (<code>global</code>).</li>
<li style="font-size:15px"><b>criteria</b>: the criteria used to select which parameters to remove (currently supported: <code>l1</code>, <code>grad</code>)</li>
<li style="font-size:15px"><b>sched_func</b>: which schedule you want to follow for the sparsification (currently supported: <a href="https://docs.fast.ai/callback.html#Annealing-functions">any scheduling function of fastai</a>, i.e <code>annealing_linear</code>, <code>annealing_cos</code>, ... and <code>annealing_gradual</code>, the schedule proposed by <a href="https://openreview.net/pdf?id=Sy1iIDkPM">Zhu & Gupta</a>{% fn 3 %}) (shown in Figure below)</li>
</ul>
</blockquote>

<br>

### Prune the network

Will *physically* remove the parameters zeroed out in step before. More information in this [blog post](https://nathanhubens.github.io/posts/deep%20learning/2020/05/22/pruning.html)

> Warning: this only works when filter sparsifying has been performed and for fully feed-forward architectures such as VGG16.

<blockquote>
<pre><b>pruner = Pruner()
pruned_model = pruner.prune_model(learn.model)</b></pre>
<p style="font-size: 15px">
You just need to pass the model whose filters has previously been sparsified and FasterAI will take care of removing them.
</p>
</blockquote>

<br>

### Batch Normalization Folding

Will remove batch normalization layers by injecting its normalization statistics (mean and variance) into the previous convolutional layer. More information in this [blog post](https://nathanhubens.github.io/posts/deep%20learning/2020/04/20/BN.html)

<blockquote>
<pre><b>bn_folder = BN_Folder()
bn_folder.fold(learn.model))</b></pre>
<p style="font-size: 15px">
Again, you only need to pass your model and FasterAI takes care of the rest. For models built using the nn.Sequential, you don't need to change anything. For others, if you want to see speedup and compression, you actually need to subclass your model to remove the batch norm from the parameters and from the <code>forward</code> method of your network.
</p>
</blockquote>

<br>

### Fully-Connected Layers Decomposition

Will replace fully-connected layers by a factorized version that is more parameter efficient.

<blockquote>
<pre><b>FCD = FCDecomposer()
decomposed_model = FCD.decompose(model, percent_removed)</b></pre>
<p style="font-size: 15px">
The <code>percent_removed</code> corresponds to the percentage of singular values removed (k value above).
</p>
</blockquote>

